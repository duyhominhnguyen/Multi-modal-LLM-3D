{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import wids\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "import torch.nn\n",
    "from random import randrange\n",
    "import os\n",
    "os.environ[\"WDS_VERBOSE_CACHE\"] = \"1\"\n",
    "os.environ[\"GOPEN_VERBOSE\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train.json\"\n",
    "\n",
    "dataset = wids.ShardListDataset(train_url)\n",
    "\n",
    "sample = dataset[1900]\n",
    "\n",
    "print(sample.keys())\n",
    "print(sample[\".txt\"])\n",
    "plt.imshow(sample[\".jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"laion/laion-art\")\n",
    "\n",
    "# Save the 'train' split of the dataset as a Parquet file\n",
    "ds[\"train\"].to_parquet(\"train_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdataset.tariterators import (\n",
    "\tbase_plus_ext,\n",
    "\ttar_file_expander,\n",
    "\turl_opener,\n",
    "\tvalid_sample,\n",
    ")\n",
    "import logging\n",
    "\n",
    "def log_and_continue(exn):\n",
    "\t\"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "\tlogging.warning(f\"Handling webdataset error ({repr(exn)}). Ignoring.\")\n",
    "\treturn True\n",
    "\n",
    "def group_by_keys_nothrow(\n",
    "\tdata, keys=base_plus_ext, lcase=True, suffixes=None, handler=None\n",
    "):\n",
    "\t\"\"\"Return function over iterator that groups key, value pairs into samples.\n",
    "\n",
    "\t:param keys: function that splits the key into key and extension (base_plus_ext)\n",
    "\t:param lcase: convert suffixes to lower case (Default value = True)\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# print(\"DATA: \", data)\n",
    "\t\n",
    "\tcurrent_sample = None\n",
    "\tfor filesample in data:\n",
    "\t\tassert isinstance(filesample, dict)\n",
    "\t\t\n",
    "\t\tfname, value = filesample[\"fname\"], filesample[\"data\"]\n",
    "\t\t\n",
    "\t\tprefix, suffix = keys(fname)\n",
    "\t\tif prefix is None:\n",
    "\t\t\tcontinue\n",
    "\t\tif lcase:\n",
    "\t\t\tsuffix = suffix.lower()\n",
    "\t\t# FIXME webdataset version throws if suffix in current_sample, but we have a potential for\n",
    "\t\t#  this happening in the current LAION400m dataset if a tar ends with same prefix as the next\n",
    "\t\t#  begins, rare, but can happen since prefix aren't unique across tar files in that dataset\n",
    "\t\tif (\n",
    "\t\t\tcurrent_sample is None\n",
    "\t\t\tor prefix != current_sample[\"__key__\"]\n",
    "\t\t\tor suffix in current_sample\n",
    "\t\t):\n",
    "\t\t\tif valid_sample(current_sample):\n",
    "\t\t\t\tyield current_sample\n",
    "\t\t\tcurrent_sample = dict(__key__=prefix, __url__=filesample[\"__url__\"])\n",
    "\t\tif suffixes is None or suffix in suffixes:\n",
    "\t\t\tcurrent_sample[suffix] = value\n",
    "\tif valid_sample(current_sample):\n",
    "\t\tyield current_sample\n",
    "\n",
    "# def tarfile_to_samples_nothrow(src, handler=log_and_continue):\n",
    "#     # NOTE this is a re-impl of the webdataset impl with group_by_keys that doesn't throw\n",
    "#     streams = url_opener(src, handler=handler)\n",
    "#     files = tar_file_expander(streams, handler=handler)\n",
    "#     # print(\"====> \\n\\n\\n Files: \", files)\n",
    "#     samples = group_by_keys_nothrow(files, handler=handler)\n",
    "#     return samples\n",
    "\n",
    "\n",
    "\n",
    "tar_src = \"/home/azureuser/maund/open_flamingo/modifications/VLM_ADNI_DATA/replicate_mmc4/000000001.tar\"\n",
    "streams = url_opener(tar_src, handler=log_and_continue)\n",
    "\n",
    "for stream in streams:\n",
    "\tprint(f\"Stream: {stream}\")\n",
    "\n",
    "files = tar_file_expander(streams, handler=log_and_continue)\n",
    "\n",
    "for file in files:\n",
    "\tprint(f\"File: {file['fname']}, Size: {len(file['data'])} bytes\")\n",
    "\n",
    "\n",
    "samples = group_by_keys_nothrow(files, handler=log_and_continue)\n",
    "samples = list(samples)\n",
    "print(samples)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anno_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/train_annotations_vqa_format.json\"\n",
    "train_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/train_questions_vqa_format.json\"\n",
    "val_anno_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/val_annotations_vqa_format.json\"\n",
    "val_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/val_questions_vqa_format.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['annotations', 'info', 'task_type', 'license', 'data_subtype'])\n",
      "dict_keys(['questions', 'info', 'task_type', 'data_type', 'license', 'data_subtype'])\n",
      "dict_keys(['annotations', 'info', 'task_type', 'license', 'data_subtype'])\n",
      "dict_keys(['questions', 'info', 'task_type', 'data_type', 'license', 'data_subtype'])\n"
     ]
    }
   ],
   "source": [
    "def reduce_size_to_n(file_name, n):\n",
    "\timport json\n",
    "\twith open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\tdata = json.load(file)\n",
    "\t\n",
    "\tprint(data.keys())\n",
    "\tif \"annotations\" in data:\n",
    "\t\tdata[\"annotations\"] = data[\"annotations\"][:n]\n",
    "\t\n",
    "\tif \"questions\" in data:\n",
    "\t\tdata[\"questions\"] = data[\"questions\"][:n]\n",
    " \n",
    "\tnew_file_name = file_name.split(\".\")[0] + f\"_{n}\" + \".\" + file_name.split(\".\")[1]\n",
    "\n",
    "\twith open(new_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "\t\tjson.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\t\n",
    "\t\t\n",
    "reduce_size_to_n(train_anno_vqa, 50)\n",
    "reduce_size_to_n(train_vqa, 50)\n",
    "reduce_size_to_n(val_anno_vqa, 50)\n",
    "reduce_size_to_n(val_vqa, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Testing Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anno_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/train_annotations_vqa_format.json\"\n",
    "train_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/train_questions_vqa_format.json\"\n",
    "val_anno_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/val_annotations_vqa_format.json\"\n",
    "val_vqa = \"/home/anhnv16/maund/open-flamingo-3D/open_flamingo/eval/data/textvqa/val_questions_vqa_format.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_images(file_name, n):\n",
    "\timport json\n",
    "\twith open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\t\tdata = json.load(file)\n",
    "  \n",
    "\tif \"annotations\" in data:\n",
    "\t\tfor anno in data[\"annotations\"]:\n",
    "\t\t\tanno[\"image_ids\"] = [anno[\"image_id\"]] * n\n",
    "\n",
    "\tif \"questions\" in data:\n",
    "\t\tfor quest in data[\"questions\"]:\n",
    "\t\t\tquest[\"image_ids\"] = [quest[\"image_id\"]] * n\n",
    "  \n",
    "\tnew_file_name = file_name.split(\".\")[0] + f\"_{n}_extended\" + \".\" + file_name.split(\".\")[1]\n",
    " \n",
    "\twith open(new_file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "\t\tjson.dump(data, file, indent=4, ensure_ascii=False)\n",
    "  \n",
    "\n",
    "add_images(train_anno_vqa, 3)\n",
    "add_images(train_vqa, 3)\n",
    "add_images(val_anno_vqa, 3)\n",
    "add_images(val_vqa, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
